{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Time Windows\n",
    "\n",
    "#Imports\n",
    "#Setup\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "###Time Windows\n",
    "#First, we will train a model to forecast the next step given the previous 20 steps, therefore, we need to create a dataset of 20-step windows for training.\n",
    "\n",
    "dataset = tf.data.Dataset.range(10)\n",
    "for val in dataset:\n",
    "    print(val.numpy())#without numpy,a bunch of tensors would be printed so numpy is added to get more readable values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 2 3 4 \n",
      "1 2 3 4 5 \n",
      "2 3 4 5 6 \n",
      "3 4 5 6 7 \n",
      "4 5 6 7 8 \n",
      "5 6 7 8 9 \n",
      "6 7 8 9 \n",
      "7 8 9 \n",
      "8 9 \n",
      "9 \n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.range(10)\n",
    "dataset = dataset.window(5, shift=1)\n",
    "for window_dataset in dataset: #Each window in this dataset is actually itself a dataset, so when we iterate over the windows \n",
    "    for val in window_dataset: # we can iterate over the elements of each window and rint its value\n",
    "        print(val.numpy(), end=\" \")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 2 3 4 \n",
      "1 2 3 4 5 \n",
      "2 3 4 5 6 \n",
      "3 4 5 6 7 \n",
      "4 5 6 7 8 \n",
      "5 6 7 8 9 \n"
     ]
    }
   ],
   "source": [
    "#To eliminate the hanging digits in the last cell and create a new data\n",
    "\n",
    "dataset = tf.data.Dataset.range(10)\n",
    "dataset = dataset.window(5, shift=1, drop_remainder=True)\n",
    "for window_dataset in dataset:\n",
    "    for val in window_dataset:\n",
    "        print(val.numpy(), end=\" \")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4]\n",
      "[1 2 3 4 5]\n",
      "[2 3 4 5 6]\n",
      "[3 4 5 6 7]\n",
      "[4 5 6 7 8]\n",
      "[5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "#we would rather have a single dataset containing batches of data in the form of regular tensors so flat map method can be used\n",
    "\n",
    "dataset = tf.data.Dataset.range(10)\n",
    "dataset = dataset.window(5, shift = 1, drop_remainder = True)\n",
    "dataset = dataset.flat_map(lambda window: (window.batch(5)))\n",
    "\"\"\"\n",
    "allows us to run a method on every single dataset that is part of nested dataset,\n",
    "i.e. a dataset that contains other datasets\n",
    "here we are going to call lambda function on every single dataset inside the nested dataset\n",
    "this lambda does is: calls window.batch a five\n",
    "In other words, it takes 5 elements from a window and creates a tensors out of them and\n",
    "since each window is 5 items long, this basically converts every single window dataset into a single tensor\n",
    "so the resulting dataset is just a dataset that contains tensors of length five\n",
    "so if iterated, a tensor of size five is produced\n",
    "\"\"\"\n",
    "for window in dataset:\n",
    "    print(window.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3] [4]\n",
      "[1 2 3 4] [5]\n",
      "[2 3 4 5] [6]\n",
      "[3 4 5 6] [7]\n",
      "[4 5 6 7] [8]\n",
      "[5 6 7 8] [9]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "for ML models, input features and labels are required\n",
    "In this case, train a forecasting model means first four elements can be features and the last one can be labels\n",
    "so a map method is used to which will transform each window intotwo tensors,\n",
    "one containing all elements except the last one and the last one containing only the last element\n",
    "A lambda function is written below to do that where map method is used to produce two tensors.\n",
    "\"\"\"\n",
    "dataset = tf.data.Dataset.range(10)\n",
    "dataset = dataset.window(5, shift=1, drop_remainder=True)\n",
    "dataset = dataset.flat_map(lambda window: window.batch(5))\n",
    "dataset = dataset.map(lambda window: (window[:-1], window[-1:]))\n",
    "for x, y in dataset:\n",
    "    print(x.numpy(), y.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 4 5 6] [7]\n",
      "[5 6 7 8] [9]\n",
      "[0 1 2 3] [4]\n",
      "[4 5 6 7] [8]\n",
      "[1 2 3 4] [5]\n",
      "[2 3 4 5] [6]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "when training a ML model, the instances in dataset are shuffled.\n",
    "This is to ensure that they are independent and identically distributed or IID,\n",
    "which is necessary, especially if you're using gradint descent, which is usually the case.\n",
    "To do that, shuffle method is called and a buffer size is specified.\n",
    "\n",
    "Each window is not shuffled. As we're ealing with a timeseries, so the elements aren't shuffled within a window\n",
    "the windows in the dataset are shuffled.\n",
    "\"\"\"\n",
    "dataset = tf.data.Dataset.range(10)\n",
    "dataset = dataset.window(5, shift=1, drop_remainder=True)\n",
    "dataset = dataset.flat_map(lambda window: window.batch(5))\n",
    "dataset = dataset.map(lambda window: (window[:-1], window[-1:]))\n",
    "dataset = dataset.shuffle(buffer_size = 10)\n",
    "for x, y in dataset:\n",
    "    print(x.numpy(), y.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = [[2 3 4 5]\n",
      " [3 4 5 6]]\n",
      "y = [[6]\n",
      " [7]]\n",
      "x = [[4 5 6 7]\n",
      " [5 6 7 8]]\n",
      "y = [[8]\n",
      " [9]]\n",
      "x = [[0 1 2 3]\n",
      " [1 2 3 4]]\n",
      "y = [[4]\n",
      " [5]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Batch method is called here to create batches of, in this case 2 windows at each training iteration.\n",
    "Prefetch method is also called to ensure that TensorFlow will load the next batch of data while it's working on the current batch of data.\n",
    "So, it never runs out of data and the Processor is kept busy as much as possible.\n",
    "\"\"\"\n",
    "dataset = tf.data.Dataset.range(10)\n",
    "dataset = dataset.window(5, shift=1, drop_remainder=True)\n",
    "dataset = dataset.flat_map(lambda window: window.batch(5))\n",
    "dataset = dataset.map(lambda window: (window[:-1], window[-1:]))\n",
    "dataset = dataset.shuffle(buffer_size=10)\n",
    "dataset = dataset.batch(2).prefetch(1)\n",
    "for x, y in dataset:\n",
    "    print(\"x =\", x.numpy())\n",
    "    print(\"y =\", y.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wrap everything in a little function called window_dataset\n",
    "def window_dataset(series, window_size, batch_size=32, # this will covert any timeseries into TensorFlow dataset that can be loaded and used for own TensorFlow models\n",
    "                   shuffle_buffer=1000):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(series)\n",
    "    dataset = dataset.window(window_size + 1, shift=1, drop_remainder=True)\n",
    "    dataset = dataset.flat_map(lambda window: window.batch(window_size + 1))\n",
    "    dataset = dataset.shuffle(shuffle_buffer)\n",
    "    dataset = dataset.map(lambda window: (window[:-1], window[-1]))\n",
    "    dataset = dataset.batch(batch_size).prefetch(1)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
